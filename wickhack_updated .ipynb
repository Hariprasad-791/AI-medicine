{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4216,"status":"ok","timestamp":1734794564373,"user":{"displayName":"Hariprasad B R","userId":"18433760805538358115"},"user_tz":-330},"id":"5EHzkogOuN6w","outputId":"30fad870-80b1-43e4-a0c5-a3f9e0f7e480"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n","Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n","Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"]}],"source":["# Install necessary libraries\n","!pip install numpy pandas scikit-learn tensorflow"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":56653,"status":"ok","timestamp":1734794560169,"user":{"displayName":"Hariprasad B R","userId":"18433760805538358115"},"user_tz":-330},"id":"3bwFG2kzB8SH","colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"e17b4585-de09-4dd3-adb2-872d59da9284"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-29eb4830-a065-46e6-b45e-1647891cff0b\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-29eb4830-a065-46e6-b45e-1647891cff0b\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving medicinedata.csv to medicinedata.csv\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization\n","from tensorflow.keras.models import Model\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report\n","import pickle\n","\n","from google.colab import files\n","\n","# Upload the updated notebook\n","uploaded = files.upload()"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1734794963057,"user":{"displayName":"Hariprasad B R","userId":"18433760805538358115"},"user_tz":-330},"id":"dsk9M_usE5dI"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Load the dataset\n","dataset_path = 'medicinedata.csv'\n","df = pd.read_csv(dataset_path)\n","\n","# Select features and labels\n","features = df[[\n","    \"Potency (%)\",\n","    \"Purity (%)\",\n","    \"Packaging_Compliance_Score (1-10)\",\n","    \"Predicted_Compliance_Score (%)\",\n","    \"Supplier_Reliability_Score (1-100)\",\n","    \"Regulatory_Benchmark_Score (%)\",\n","    \"Historical_Quality_Score (%)\"\n","]]\n","\n","labels = df[\"Anomaly_Flag\"]\n","\n","# Normalize the features\n","scaler = MinMaxScaler()\n","normalized_features = scaler.fit_transform(features)\n","\n","# Split into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    normalized_features, labels, test_size=0.2, random_state=42, stratify=labels\n",")\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":11774,"status":"ok","timestamp":1734795579579,"user":{"displayName":"Hariprasad B R","userId":"18433760805538358115"},"user_tz":-330},"id":"92-CfIt1FJK8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"05a38134-8bca-4731-bb8e-f8e6f7f0cda6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 136ms/step - loss: 0.7143 - val_loss: 0.7899\n","Epoch 2/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.6770 - val_loss: 0.7525\n","Epoch 3/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.6476 - val_loss: 0.6999\n","Epoch 4/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.6346 - val_loss: 0.7197\n","Epoch 5/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.6480 - val_loss: 0.7303\n","Epoch 6/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6114 - val_loss: 0.7242\n","Epoch 7/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.6319 - val_loss: 0.7205\n","Epoch 8/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.6258 - val_loss: 0.7162\n","Epoch 9/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.6170 - val_loss: 0.7113\n","Epoch 10/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.6336 - val_loss: 0.7106\n","Epoch 11/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.6357 - val_loss: 0.7286\n","Epoch 12/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.6098 - val_loss: 0.7164\n","Epoch 13/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.6238 - val_loss: 0.6993\n","Epoch 14/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.6212 - val_loss: 0.6998\n","Epoch 15/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.6323 - val_loss: 0.7150\n","Epoch 16/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.6162 - val_loss: 0.7435\n","Epoch 17/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.6362 - val_loss: 0.7497\n","Epoch 18/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.6389 - val_loss: 0.7204\n","Epoch 19/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.6182 - val_loss: 0.7192\n","Epoch 20/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.6052 - val_loss: 0.7299\n","Epoch 21/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.6302 - val_loss: 0.7436\n","Epoch 22/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.6228 - val_loss: 0.6978\n","Epoch 23/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.6239 - val_loss: 0.7174\n","Epoch 24/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.6141 - val_loss: 0.7088\n","Epoch 25/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6106 - val_loss: 0.7331\n","Epoch 26/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6395 - val_loss: 0.7111\n","Epoch 27/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.6248 - val_loss: 0.7413\n","Epoch 28/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.6088 - val_loss: 0.6996\n","Epoch 29/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.6194 - val_loss: 0.7182\n","Epoch 30/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.6215 - val_loss: 0.7116\n","Epoch 31/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.6314 - val_loss: 0.7221\n","Epoch 32/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.6151 - val_loss: 0.7222\n","Epoch 33/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.6217 - val_loss: 0.6990\n","Epoch 34/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.6237 - val_loss: 0.7314\n","Epoch 35/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.6327 - val_loss: 0.7240\n","Epoch 36/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.6145 - val_loss: 0.7265\n","Epoch 37/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.6257 - val_loss: 0.7197\n","Epoch 38/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.6154 - val_loss: 0.7169\n","Epoch 39/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.6264 - val_loss: 0.7050\n","Epoch 40/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.6190 - val_loss: 0.7092\n","Epoch 41/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.6392 - val_loss: 0.7240\n","Epoch 42/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.6301 - val_loss: 0.7356\n","Epoch 43/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.6217 - val_loss: 0.7375\n","Epoch 44/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.6154 - val_loss: 0.7221\n","Epoch 45/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.6147 - val_loss: 0.7218\n","Epoch 46/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.6310 - val_loss: 0.7390\n","Epoch 47/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.6347 - val_loss: 0.7194\n","Epoch 48/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.6224 - val_loss: 0.7094\n","Epoch 49/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.6391 - val_loss: 0.7217\n","Epoch 50/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.6195 - val_loss: 0.7357\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7b9d6d01cfa0>"]},"metadata":{},"execution_count":12}],"source":["from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization\n","from tensorflow.keras.models import Model\n","import tensorflow as tf\n","\n","# Define the latent space dimension\n","latent_dim = 2\n","\n","# Encoder\n","inputs = Input(shape=(7,), name=\"encoder_input\")  # Updated input shape to match dataset\n","x = Dense(64, activation='relu')(inputs)\n","x = BatchNormalization()(x)\n","x = Dense(32, activation='relu')(x)\n","x = BatchNormalization()(x)\n","\n","z_mean = Dense(latent_dim, name=\"z_mean\")(x)\n","z_log_var = Dense(latent_dim, name=\"z_log_var\")(x)\n","\n","# Sampling function\n","def sampling(args):\n","    z_mean, z_log_var = args\n","    epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], latent_dim), mean=0., stddev=1.0)\n","    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n","\n","z = Lambda(sampling, output_shape=(latent_dim,), name=\"z\")([z_mean, z_log_var])\n","\n","# Decoder\n","decoder_inputs = Input(shape=(latent_dim,), name=\"decoder_input\")\n","x = Dense(32, activation='relu')(decoder_inputs)\n","x = BatchNormalization()(x)\n","x = Dense(64, activation='relu')(x)\n","x = BatchNormalization()(x)\n","outputs = Dense(7, activation='sigmoid', name=\"decoder_output\")(x)  # Updated output shape to match dataset\n","\n","# Encoder and Decoder models\n","encoder = Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")\n","decoder = Model(decoder_inputs, outputs, name=\"decoder\")\n","\n","# Full VAE model\n","vae_outputs = decoder(encoder(inputs)[2])\n","\n","# Define custom VAE model with integrated loss\n","class VAE(Model):\n","    def __init__(self, encoder, decoder, **kwargs):\n","        super(VAE, self).__init__(**kwargs)\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def call(self, inputs):\n","        z_mean, z_log_var, z = self.encoder(inputs)\n","        reconstructed = self.decoder(z)\n","        # Reconstruction loss\n","        reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.square(inputs - reconstructed), axis=1))\n","        # KL divergence loss\n","        kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1))\n","        self.add_loss(reconstruction_loss + kl_loss)\n","        return reconstructed\n","\n","vae = VAE(encoder, decoder)\n","vae.compile(optimizer=tf.keras.optimizers.Adam())\n","\n","# Train the model\n","vae.fit(X_train, X_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1133,"status":"ok","timestamp":1734795588576,"user":{"displayName":"Hariprasad B R","userId":"18433760805538358115"},"user_tz":-330},"id":"LIMPq8WQO-_Y","outputId":"a0f1f022-3d30-4ab0-cd04-ceec277219ef"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Model and scaler saved successfully!\n"]}],"source":["vae.save(\"vae_anomaly_detection_model.h5\")\n","\n","# Save scaler for deployment\n","with open(\"scaler.pkl\", \"wb\") as f:\n","    pickle.dump({\"scaler\": scaler}, f)\n","\n","print(\"Model and scaler saved successfully!\")\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3268,"status":"ok","timestamp":1734795904791,"user":{"displayName":"Hariprasad B R","userId":"18433760805538358115"},"user_tz":-330},"id":"_KmBY-NqgWOZ","outputId":"fa54cd6f-58a1-4ca9-ecab-c0dac4a572ea"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 809ms/step\n","Anomaly Detection Results:\n","Instance 2 is an anomaly. Significant deviations in attributes: [0 1 2 3 4 5 6]\n","Instance 23 is an anomaly. Significant deviations in attributes: [0 1 2 3 4 5 6]\n","Instance 31 is an anomaly. Significant deviations in attributes: [0 1 2 3 4 5 6]\n","\n","Test dataset results saved as 'test_dataset_results.csv'.\n"]}],"source":["# Generate a separate test dataset with 7 features (to match the training dataset)\n","np.random.seed(42)\n","test_data = np.vstack([\n","    np.random.normal(loc=0.5, scale=0.1, size=(50, 7)),  # Normal data\n","    np.random.normal(loc=1.5, scale=0.2, size=(10, 7))   # Data with anomalies\n","])\n","\n","# Normalize the test data using the same scaler\n","test_data = scaler.transform(test_data)\n","\n","# Test the VAE on the new test dataset\n","reconstructed = vae.predict(test_data)\n","reconstruction_errors = np.mean(np.square(test_data - reconstructed), axis=1)\n","\n","# Determine threshold for anomaly detection\n","threshold = np.percentile(reconstruction_errors, 95)  # Use 95th percentile\n","\n","# Classify anomalies based on reconstruction error\n","predictions = (reconstruction_errors > threshold).astype(int)\n","\n","# Identify anomalies and highlight tuples\n","anomalous_tuples = []\n","anomalous_attributes = []\n","for i, (original, recon) in enumerate(zip(test_data, reconstructed)):\n","    if predictions[i] == 1:  # If the instance is an anomaly\n","        attribute_diff = np.abs(original - recon)\n","        significant_attributes = np.where(attribute_diff > 0.2)[0]  # Threshold for significant deviation\n","        anomalous_tuples.append((i, original))\n","        anomalous_attributes.append((i, significant_attributes))\n","\n","# Output results\n","print(\"Anomaly Detection Results:\")\n","for i, attributes in anomalous_attributes:\n","    print(f\"Instance {i} is an anomaly. Significant deviations in attributes: {attributes}\")\n","\n","# Save the test dataset and results\n","import pandas as pd\n","test_df = pd.DataFrame(test_data, columns=[f\"Feature_{i+1}\" for i in range(test_data.shape[1])])\n","test_df[\"Anomaly\"] = predictions\n","test_df.to_csv(\"test_dataset_results.csv\", index=False)\n","\n","print(\"\\nTest dataset results saved as 'test_dataset_results.csv'.\")\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":229},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1734796065345,"user":{"displayName":"Hariprasad B R","userId":"18433760805538358115"},"user_tz":-330},"id":"J_-XNc_5gcJy","outputId":"a5d2ef8b-9f3f-4427-b702-d8fc07495b49"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\n","Tuple Evaluation Results:\n","Input Tuple: [0.6, 0.7, 0.5, 0.8, 0.4, 0.9, 0.5]\n","Reconstruction Error: 67.8078\n","Anomaly Status: Normal\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["\"\\n'Chemical_Composition_Compliance (%): 60% normalized compliance.\\nPackaging_Compliance_Score (1-10): 70% normalized compliance.\\nRegulatory_Benchmark_Score (%): 50% normalized compliance.\\nPredicted_Compliance_Score (%): 80% normalized prediction.\\nPotency (%): 40% normalized strength.\\nPurity (%): 90% normalized quality.\\nSupplier_Reliability_Score (1-100): 50% normalized reliability..\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}],"source":["# Function to evaluate a specific tuple\n","def evaluate_tuple(input_tuple):\n","    # Normalize the input tuple using the same scaler\n","    normalized_tuple = scaler.transform([input_tuple])\n","\n","    # Reconstruct using the VAE\n","    reconstructed_tuple = vae.predict(normalized_tuple)\n","    reconstruction_error = np.mean(np.square(normalized_tuple - reconstructed_tuple))\n","\n","    # Check if it's an anomaly\n","    is_anomalous = reconstruction_error > threshold\n","    significant_attributes = np.where(np.abs(normalized_tuple - reconstructed_tuple) > 0.2)[1]\n","\n","    print(\"\\nTuple Evaluation Results:\")\n","    print(f\"Input Tuple: {input_tuple}\")\n","    print(f\"Reconstruction Error: {reconstruction_error:.4f}\")\n","    print(f\"Anomaly Status: {'Anomalous' if is_anomalous else 'Normal'}\")\n","    if is_anomalous:\n","        print(f\"Significant Deviations in Attributes: {significant_attributes.tolist()}\")\n","\n","# Example usage for a specific tuple with 7 features\n","# Replace this with your actual features (ensuring 7 values)\n","sample_tuple = [0.6, 0.7, 0.5, 0.8, 0.4, 0.9, 0.5]\n","evaluate_tuple(sample_tuple)\n","\n","# Example usage for a specific tuple\n","'''\n","'Chemical_Composition_Compliance (%): 60% normalized compliance.\n","Packaging_Compliance_Score (1-10): 70% normalized compliance.\n","Regulatory_Benchmark_Score (%): 50% normalized compliance.\n","Predicted_Compliance_Score (%): 80% normalized prediction.\n","Potency (%): 40% normalized strength.\n","Purity (%): 90% normalized quality.\n","Supplier_Reliability_Score (1-100): 50% normalized reliability..\n","'''\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}